{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17f5d665-5417-413f-8ada-eac6fcf30210",
   "metadata": {},
   "source": [
    "# Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802b7c24-8c15-4ea0-9fcb-e1b7544a85cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "import importlib\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# IMPORT TORCH\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# IMPORT SELFEEG \n",
    "import selfeeg.dataloading as dl\n",
    "\n",
    "# IMPORT REPOSITORY FUNCTIONS\n",
    "import AllFnc\n",
    "from   AllFnc import split\n",
    "from   AllFnc.models import ShallowNet2\n",
    "from   AllFnc.training import loadEEG\n",
    "\n",
    "# IMPORT EEGVISLIB\n",
    "from   AllFnc import eegvislib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message = \"Using padding='same'\", category = UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category = FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e1b8c2-8173-4ce1-baa1-1960ffa876bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "# SET PATH\n",
    "sep         = os.path.sep\n",
    "dataPath    = '/data/delpup/datasets/eegpickle/'\n",
    "osPath      = os.path.abspath(os.getcwd())\n",
    "imgPath     = osPath + sep + 'imgs' + sep\n",
    "modelsPath  = osPath + sep + 'AlzClassification' + sep + 'Models' + sep\n",
    "resultsPath = osPath + sep + 'AlzClassification' + sep + 'Results' + sep\n",
    "\n",
    "# IMGS OPTIONS\n",
    "imgs_format = '.pdf'\n",
    "save_img    = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88296001-c3f7-4176-87fd-068d688ff91a",
   "metadata": {},
   "source": [
    "## Set training spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3d54d2-202b-4b71-a001-d989a5180bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineToEval = 'filt' #'icasr'\n",
    "taskToEval     = 'alzheimer'\n",
    "outFold        = 6\n",
    "inFold         = 5\n",
    "downsample     = True\n",
    "z_score        = True\n",
    "rem_interp     = True\n",
    "batchsize      = 64\n",
    "overlap        = 0.0\n",
    "workers        = 0\n",
    "window         = 4\n",
    "verbose        = True\n",
    "arch_acronym   = 'shn7db'\n",
    "seed           = 83136297\n",
    "\n",
    "modelToimport = 'alz_flt_125_shn7db_006_005_000050_019_004'\n",
    "\n",
    "# ShallowNet\n",
    "# shallownet_custom_dict = {\n",
    "#     \"F1\": 40,\n",
    "#     \"K1\": 25,\n",
    "#     \"F2\": 40,\n",
    "#     \"Pool\": 75,\n",
    "#     \"p\": 0.2,\n",
    "#     \"log_activation_base\": \"e\",\n",
    "#     \"norm_type\": \"batchnorm\",\n",
    "#     \"random_temporal_filter\": True,\n",
    "#     \"Fs\": 125 if downsample else 250 ,\n",
    "#     \"freeze_temporal\": 0,\n",
    "#     \"dense_hidden\": None,\n",
    "#     \"spatial_depthwise\": False,\n",
    "#     \"spatial_only_positive\": False,\n",
    "#     \"global_pooling\": False,\n",
    "#     \"bias\": [True, True, True],\n",
    "#     \"return_logits\": True,\n",
    "#     \"seed\": 83136297\n",
    "# }\n",
    "\n",
    "# Med-ShallowNet\n",
    "shallownet_custom_dict = {\n",
    "    \"F1\": 7,\n",
    "    \"K1\": 125,\n",
    "    \"F2\": 7,\n",
    "    \"Pool\": 75,\n",
    "    \"p\": 0.2,\n",
    "    \"log_activation_base\": \"dB\",\n",
    "    \"norm_type\": \"batchnorm\",\n",
    "    \"random_temporal_filter\": False,\n",
    "    \"Fs\": 125 if downsample else 250 ,\n",
    "    \"freeze_temporal\": 999999999,\n",
    "    \"dense_hidden\": None,\n",
    "    \"spatial_depthwise\": True,\n",
    "    \"spatial_only_positive\": False,\n",
    "    \"global_pooling\": True,\n",
    "    \"bias\": [False, False, False],\n",
    "    \"return_logits\": True,\n",
    "    \"seed\": seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f76699-15f2-4664-85c4-90f7501182fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold to eval is the correct index to get the desired train/val/test part\n",
    "outerFold  = outFold - 1\n",
    "innerFold  = inFold  - 1\n",
    "foldToEval = outerFold*5 + innerFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cd00e-2936-450c-977d-d40604364be3",
   "metadata": {},
   "source": [
    "## Create partition list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e2cdff-120f-4808-aaa9-2bfc19e4109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'alzheimer' in taskToEval.casefold():\n",
    "    # ALZ = subjects 1 to 36; CTL = subjects 37 to 65; FTD = subjects 66 to 88\n",
    "    a_id = [i for i in range(1,37)]\n",
    "    c_id = [i for i in range(37,66)]\n",
    "    f_id = [i for i in range(66,89)]\n",
    "    \n",
    "    part_a = split.create_nested_kfold_subject_split(a_id, 10, 5)\n",
    "    part_c = split.create_nested_kfold_subject_split(c_id, 10, 5)\n",
    "    part_f = split.create_nested_kfold_subject_split(f_id, 10, 5)\n",
    "\n",
    "    if taskToEval.casefold() == 'alzheimer':\n",
    "        partition_list_1 = split.merge_partition_lists(part_a, part_c, 10, 5)\n",
    "        partition_list = split.merge_partition_lists(partition_list_1, part_f, 10, 5)\n",
    "    elif taskToEval.casefold() == 'alzheimerca':\n",
    "        partition_list = split.merge_partition_lists(part_a, part_c, 10, 5)\n",
    "    elif taskToEval.casefold() == 'alzheimercf':\n",
    "        partition_list = split.merge_partition_lists(part_c, part_f, 10, 5)\n",
    "    elif taskToEval.casefold() == 'alzheimeraf':\n",
    "        partition_list = split.merge_partition_lists(part_a, part_f, 10, 5)\n",
    "\n",
    "elif taskToEval.casefold() == 'cognitive':\n",
    "    # CTL = subjects 101 to 149; PD/PDD/PDMCI = mixing number in [1; 100]\n",
    "    c_id = [i for i in range(101,150)]\n",
    "    pd_id = [3, 6, 7, 16, 17, 18, 21, 24, 26, 27, 30, 32, 35, 37, 39, 40, 45,\n",
    "             46, 50, 51, 53, 57, 58, 60, 61, 62, 63, 65, 67, 68, 69, 71, 74,\n",
    "             76, 80, 81, 82, 83, 84, 85, 86, 87, 90, 92, 93, 94, 100]\n",
    "    pdd_id = [1, 2, 5, 8, 9, 12, 13, 15, 22, 25, 33, 38, 44, 48, 75, 78, 88, 95, 96]\n",
    "    pdmci_id = [4, 10, 11, 14, 19, 20, 23, 28, 29, 31, 34, 36, 41, 42, 43, 47,\n",
    "                49, 52, 54, 55, 56, 59, 64, 66, 70, 72, 73, 77, 79, 89, 91, 97,\n",
    "                98, 99]\n",
    "    part_c = split.create_nested_kfold_subject_split(c_id, 10, 5)\n",
    "    part_p = split.create_nested_kfold_subject_split(pd_id, 10, 5)\n",
    "    part_d = split.create_nested_kfold_subject_split(pdd_id, 10, 5)\n",
    "    part_m = split.create_nested_kfold_subject_split(pdmci_id, 10, 5)\n",
    "    \n",
    "    # first --> mix two groups; then --> mix the mix\n",
    "    # splits have a more similar number of subject per set in this way\n",
    "    partition_1 = split.merge_partition_lists(part_c, part_m, 10, 5)\n",
    "    partition_2 = split.merge_partition_lists(part_p, part_d, 10, 5)\n",
    "    partition_list = split.merge_partition_lists(partition_1, partition_2, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90352f5-9a32-4af2-add6-91d9368cd17e",
   "metadata": {},
   "source": [
    "## Get other dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb580170-767d-4cd6-8936-cc18fa860ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Path to EEG data as a concatenation of:\n",
    "# 1) the root path\n",
    "# 2) the preprocessing pipeline\n",
    "if dataPath[-1] != os.sep:\n",
    "    dataPath += os.sep\n",
    "if pipelineToEval[-1] != os.sep:\n",
    "    eegpath = dataPath + pipelineToEval + os.sep\n",
    "else:\n",
    "    eegpath = dataPath + pipelineToEval\n",
    "\n",
    "# Define the number of Channels to use. \n",
    "# Basically 61 due to BIDSAlign channel system alignment.\n",
    "# Note that BIDSAlign DOES NOT delete any original channel by default.\n",
    "if rem_interp:\n",
    "    if 'alzheimer' in taskToEval.casefold():\n",
    "        Chan = 19\n",
    "    elif taskToEval.casefold() == 'cognitive':\n",
    "        Chan = 59\n",
    "else:\n",
    "    Chan = 61\n",
    "\n",
    "# Define the sampling rate. 125 or 250 depending on the downsample option\n",
    "srate = 125 if downsample else 250\n",
    "\n",
    "# Define the number of classes to predict.\n",
    "# All tasks are binary except the Alzheimer's one, \n",
    "# which is a multi-class classification (Alzheimer vs FrontoTemporal vs Control)\n",
    "if taskToEval.casefold() == 'alzheimer':\n",
    "    nb_classes = 3\n",
    "elif taskToEval.casefold() == 'cognitive':\n",
    "    nb_classes = 4\n",
    "else:\n",
    "    nb_classes = 2\n",
    "\n",
    "# For selfEEG's models instantiation\n",
    "Samples = int(srate*window)\n",
    "\n",
    "# Set the Dataset ID for glob.glob operation in SelfEEG's GetEEGPartitionNumber().\n",
    "if 'alzheimer' in taskToEval.casefold():\n",
    "    datasetID = '10'\n",
    "elif taskToEval.casefold() == 'cognitive':\n",
    "    datasetID = '19'\n",
    "\n",
    "# Set the class label in case of plot of functions\n",
    "if taskToEval.casefold() == 'alzheimer':\n",
    "    classlabels = ['CTL', 'FTD', 'AD']\n",
    "elif taskToEval.casefold() == 'alzheimerca':\n",
    "    classlabels = ['CTL', 'AD']\n",
    "elif taskToEval.casefold() == 'alzheimercf':\n",
    "    classlabels = ['CTL', 'FTD']\n",
    "elif taskToEval.casefold() == 'alzheimeraf':\n",
    "    classlabels = ['FTD', 'AD']\n",
    "elif taskToEval.casefold() == 'cognitive':\n",
    "    classlabels = ['CTL', 'PD', 'PDD', 'PDMCI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98384d-b520-4753-9bf4-a3edd2e3542a",
   "metadata": {},
   "source": [
    "## Define torch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b949c8-8648-4799-aa0a-d4a2571fbb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "loadEEG_args = {'return_label': False, \n",
    "                'downsample': downsample, \n",
    "                'use_only_original': rem_interp,\n",
    "                'apply_zscore': z_score}\n",
    "\n",
    "glob_input = [datasetID + '_*.pickle']\n",
    "\n",
    "# calculate dataset length.\n",
    "# Basically it automatically retrieves all the partitions \n",
    "# that can be extracted from each EEG signal\n",
    "EEGlen = dl.get_eeg_partition_number(\n",
    "    eegpath,\n",
    "    srate,\n",
    "    window,\n",
    "    overlap, \n",
    "    file_format = glob_input,\n",
    "    load_function = loadEEG,\n",
    "    optional_load_fun_args = loadEEG_args,\n",
    "    includePartial = False if overlap == 0 else True,\n",
    "    verbose = verbose\n",
    ")\n",
    "\n",
    "# Now we also need to load the labels\n",
    "loadEEG_args['return_label'] = True\n",
    "\n",
    "# Set functions to retrieve dataset, subject, and session from each filename.\n",
    "# They will be used by GetEEGSplitTable to perform a subject based split\n",
    "dataset_id_ex  = lambda x: int(x.split(os.sep)[-1].split('_')[0])\n",
    "subject_id_ex  = lambda x: int(x.split(os.sep)[-1].split('_')[1]) \n",
    "session_id_ex  = lambda x: int(x.split(os.sep)[-1].split('_')[2]) \n",
    "\n",
    "# Now call the GetEEGSplitTable. \n",
    "if taskToEval.casefold() == 'alzheimerca':\n",
    "    exclude_id = f_id\n",
    "elif taskToEval.casefold() == 'alzheimercf':\n",
    "    exclude_id = a_id\n",
    "elif taskToEval.casefold() == 'alzheimeraf':\n",
    "    exclude_id = c_id\n",
    "else:\n",
    "    exclude_id = None\n",
    "        \n",
    "    train_id   = partition_list[foldToEval][0]\n",
    "    val_id     = partition_list[foldToEval][1]\n",
    "    test_id    = partition_list[foldToEval][2]\n",
    "    EEGsplit = dl.get_eeg_split_table(\n",
    "        partition_table      = EEGlen,\n",
    "        val_data_id          = val_id,\n",
    "        test_data_id         = test_id,\n",
    "        exclude_data_id      = exclude_id,\n",
    "        split_tolerance      = 0.001,\n",
    "        dataset_id_extractor = subject_id_ex,\n",
    "        subject_id_extractor = session_id_ex,\n",
    "        perseverance         = 10000)\n",
    "\n",
    "if verbose:\n",
    "    print(' ')\n",
    "    print('Subjects used for test')\n",
    "    print(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740546fa-6167-4e17-8f66-730eb03386f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Datasets and preload all data\n",
    "trainset = dl.EEGDataset(\n",
    "    EEGlen, EEGsplit, [srate, window, overlap], 'train', \n",
    "    supervised             = True, \n",
    "    label_on_load          = True,\n",
    "    load_function          = loadEEG,\n",
    "    optional_load_fun_args = loadEEG_args\n",
    ")\n",
    "trainset.preload_dataset()\n",
    "\n",
    "valset = dl.EEGDataset(\n",
    "    EEGlen, EEGsplit, [srate, window, overlap], 'validation',\n",
    "    supervised             = True, \n",
    "    label_on_load          = True,\n",
    "    load_function          = loadEEG,\n",
    "    optional_load_fun_args = loadEEG_args\n",
    ")\n",
    "valset.preload_dataset()\n",
    "\n",
    "testset = dl.EEGDataset(\n",
    "    EEGlen, EEGsplit, [srate, window, overlap], 'test',\n",
    "    supervised             = True,\n",
    "    label_on_load          = True,\n",
    "    load_function          = loadEEG,\n",
    "    optional_load_fun_args = loadEEG_args\n",
    ")\n",
    "testset.preload_dataset()\n",
    "\n",
    "# Convert to long if task is multiclass classification.\n",
    "# This avoids Value Errors during cross entropy loss calculation\n",
    "if ('alzheimer' in taskToEval.casefold()) or ('cognitive' in taskToEval.casefold()):\n",
    "    if taskToEval.casefold() == 'alzheimerca':\n",
    "        trainset.y_preload[trainset.y_preload==2] = 1\n",
    "        valset.y_preload[valset.y_preload==2] = 1 \n",
    "        testset.y_preload[testset.y_preload==2] = 1\n",
    "        \n",
    "    elif taskToEval.casefold() == 'alzheimeraf':\n",
    "        trainset.y_preload -= 1\n",
    "        valset.y_preload   -= 1 \n",
    "        testset.y_preload  -= 1\n",
    "        \n",
    "    elif taskToEval.casefold() == 'alzheimeraf':\n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        trainset.y_preload = trainset.y_preload.to(dtype = torch.long)\n",
    "        valset.y_preload   = valset.y_preload.to(dtype = torch.long)\n",
    "        testset.y_preload  = testset.y_preload.to(dtype = torch.long)\n",
    "    \n",
    "# Finally, Define Dataloaders\n",
    "# (no need to use more workers in validation and test dataloaders)\n",
    "trainloader = DataLoader(dataset = trainset, batch_size = batchsize,\n",
    "                         shuffle = True, num_workers = workers)\n",
    "valloader = DataLoader(dataset = valset, batch_size = batchsize,\n",
    "                       shuffle = False, num_workers = 0)\n",
    "testloader = DataLoader(dataset = testset, batch_size = batchsize,\n",
    "                        shuffle = False, num_workers = 0)\n",
    "\n",
    "if verbose:\n",
    "    # plot split statistics\n",
    "    labels = np.zeros(len(EEGlen))\n",
    "    for i in range(len(EEGlen)):\n",
    "        path = EEGlen.iloc[i,0]\n",
    "        with open(path, 'rb') as eegfile:\n",
    "            EEG = pickle.load(eegfile)\n",
    "        labels[i] = EEG['label']\n",
    "    dl.check_split(EEGlen, EEGsplit, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1047e88-c9c1-4742-b5a6-d9a887f736d5",
   "metadata": {},
   "source": [
    "## Import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a506018a-78c1-410d-81fd-01025047f4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# define model\n",
    "Mdl     = ShallowNet2(nb_classes, Chan, Samples, **shallownet_custom_dict)\n",
    "Mdl_weights = torch.load(modelsPath + modelToimport + '.pt')\n",
    "Mdl.load_state_dict(Mdl_weights)\n",
    "Mdl_weights = {k: v for k, v in Mdl.state_dict().items()}\n",
    "Mdl_weights = dict(Mdl_weights)\n",
    "\n",
    "MdlBase = copy.deepcopy(Mdl)\n",
    "MdlBase_weights = {k: v for k, v in MdlBase.state_dict().items()}\n",
    "MdlBase_weights = dict(MdlBase_weights)\n",
    "\n",
    "with open(resultsPath + modelToimport+'.pickle', 'rb') as file:\n",
    "    scores = pickle.load(file)\n",
    "\n",
    "inout_labels = [[classlabels[int(i)] for i in scores['labels']],\n",
    "                [classlabels[int(i)] for i in scores['predictions']]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92d6e7e-4c1e-4f9d-99cf-3c69093f2598",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db2fad06-71a3-414f-a500-ee73d551fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flim = [1,45]\n",
    "\n",
    "bands   = {'$\\\\delta$' : [flim[0],4],\n",
    "           '$\\\\theta$' : [4,8],\n",
    "           '$\\\\alpha$' : [8,12],\n",
    "           '$\\\\beta_1$': [12,16],\n",
    "           '$\\\\beta_2$': [16,20],\n",
    "           '$\\\\beta_3$': [20,28],\n",
    "           '$\\\\gamma$' : [28,flim[1]]}\n",
    "\n",
    "channels = ['Fp1','Fp2','F7','F3','Fz',\n",
    "            'F4','F8','T7','C3','Cz',\n",
    "            'C4','T8','P7','P3','Pz',\n",
    "            'P4','P8','O1','O2']\n",
    "\n",
    "montage = 'biosemi64'\n",
    "\n",
    "spec_dict_keys  = ['bands','channels','montage','flim', 'srate', 'classlabels']\n",
    "spec_dict_bands = [ bands,  channels,  montage,  flim,   srate,   classlabels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1256d4b6-39e9-4a7e-ab1c-08d30d9573df",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_window_normalized = testloader.dataset[:][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64958701-6929-4dbc-8af4-40c2af9e777b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d3dc51-0fc4-42d0-8e05-3e97ec999cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_IDs = [100, 1000]\n",
    "\n",
    "spec_dict = eegvislib.get_spec_dict(spec_dict_keys, spec_dict_bands)\n",
    "spec_dict['figdim']   = [5,5]\n",
    "spec_dict['font']     = 30\n",
    "spec_dict['title']    = 'Input Signal'\n",
    "spec_dict['cbartick'] = 5\n",
    "spec_dict['f_des']    = [1,6,10,15,18,20,45]\n",
    "spec_dict['contours'] = 0\n",
    "spec_dict['lognorm']  = '10log10'\n",
    "spec_dict['vlim']     = 'all' # 'all' or 'band' or 'batchID'\n",
    "spec_dict['cmap']     = 'RdBu_r'\n",
    "spec_dict['figname']  = 'scalpPSD'\n",
    "\n",
    "for batch_ID in batch_IDs:\n",
    "    fig = eegvislib.scalp_plot(input_window_normalized, \n",
    "                               spec_dict, inout_labels,\n",
    "                               batch_ID)\n",
    "    \n",
    "    if save_img:\n",
    "        fig.savefig(imgPath + spec_dict['figname'] + f'_batch{batch_ID}' + f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                    transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90217f13-7aa7-4812-b701-0225b01b171d",
   "metadata": {},
   "source": [
    "### Conv1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1371b972-b207-45e7-a2cc-ddd2d1d2a27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'encoder.conv1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb96d8a-67ec-4465-8195-5e68fbc8fb2b",
   "metadata": {},
   "source": [
    "KERNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e2bb4f-24f4-4e5f-af6d-f596384a4b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ID     = [0,1,2,3,4,5,6] #None (all Kernels), a number 0, or a list [0], [0,2]\n",
    "kernel_ID     = 0\n",
    "kernel_height = 0\n",
    "\n",
    "spec_dict = eegvislib.get_spec_dict(spec_dict_keys, spec_dict_bands)\n",
    "spec_dict['figdim']      = [5,5]\n",
    "spec_dict['linew']       = 3\n",
    "spec_dict['font']        = 20\n",
    "spec_dict['title']       = 'First Layer Filters ShallowNet'\n",
    "spec_dict['rotation']    = 45\n",
    "spec_dict['lognorm']     = '10log10'\n",
    "spec_dict['bodeplot']    = 'mag' #both, phase, mag\n",
    "spec_dict['colors']      = [['tab:red','black'],['tab:orange','tab:blue']]\n",
    "spec_dict['linestyle']   = ['-','--']\n",
    "spec_dict['loc']         = 'best'\n",
    "spec_dict['filternames'] = [i for i in spec_dict['bands'].keys()] #or None\n",
    "spec_dict['figname']     = 'firstLayerFiltersBodeplot'\n",
    "\n",
    "fig, ax = eegvislib.BP_temporalkernels(Mdl_weights, layer,\n",
    "                                       spec_dict, \n",
    "                                       filter_ID, kernel_ID, kernel_height, \n",
    "                                       None)\n",
    "\n",
    "if save_img:\n",
    "    fig.savefig(imgPath + spec_dict['figname']+ f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f0ddc-ac5b-4868-9789-d9a71fb86076",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ID     = None\n",
    "kernel_ID     = 0\n",
    "kernel_height = 0\n",
    "\n",
    "spec_dict = eegvislib.get_spec_dict(spec_dict_keys, spec_dict_bands)\n",
    "spec_dict['figdim']  = [3,3]\n",
    "spec_dict['font']    = 14\n",
    "spec_dict['ytick']   = 5\n",
    "spec_dict['s']       = 2\n",
    "spec_dict['cmap']    = 'RdBu_r'\n",
    "spec_dict['figname'] = 'firstLayerFiltersStemplot'\n",
    "\n",
    "fig = eegvislib.weights_temporalkernels(Mdl_weights, layer, \n",
    "                                        spec_dict, \n",
    "                                        filter_ID, kernel_ID, kernel_height)\n",
    "\n",
    "if save_img:\n",
    "    fig.savefig(imgPath + spec_dict['figname']+ f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd49f30-f4ce-49f6-8f36-0b7617d785ab",
   "metadata": {},
   "source": [
    "ACTIVATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2c912b-1606-42b0-8c9a-49c674e676fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "convoluted_output  = eegvislib.out_activation(Mdl, layer, input_window_normalized)\n",
    "convoluted_output.shape\n",
    "#batch x feature maps x scalp channels x time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f0edc-1f05-4ada-9529-cfbef415463c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "chan_ID   = ['Fp1'] #or multiple channels ['Oz','Fpz']\n",
    "batch_IDs = [100, 1000]\n",
    "FM_ID     = [0,1,2,3,4,5,6] #None (all FM) or a number 0 or list [0], [2,4]\n",
    "\n",
    "spec_dict = eegvislib.get_spec_dict(spec_dict_keys, spec_dict_bands)\n",
    "spec_dict['figdim']  = [4,3]\n",
    "spec_dict['linew']   = 2\n",
    "spec_dict['font']    = 14\n",
    "spec_dict['lognorm'] = '10log10'\n",
    "spec_dict['loc']     = 'upper right'\n",
    "spec_dict['figname'] = 'firstLayerActivationsChannels'\n",
    "\n",
    "for batch_ID in batch_IDs:\n",
    "    fig = eegvislib.temporalFM_PSD(input_window_normalized, \n",
    "                                   Mdl, layer,   \n",
    "                                   spec_dict, inout_labels,\n",
    "                                   chan_ID, batch_ID, FM_ID)\n",
    "    \n",
    "    if save_img:\n",
    "        fig.savefig(imgPath + spec_dict['figname'] + f'_batch{batch_ID}'+ f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                    transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2ce40f-0d28-4872-b2a2-ed35638eede8",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_IDs = [100, 1000]\n",
    "FM_ID     = [3] #None (all FM) or a number 0 or list [0], [2,4]\n",
    "\n",
    "spec_dict = eegvislib.get_spec_dict(spec_dict_keys, spec_dict_bands)\n",
    "spec_dict['figdim']   = [5,5]\n",
    "spec_dict['font']     = 30\n",
    "spec_dict['cbartick'] = 5\n",
    "spec_dict['contours'] = 0  \n",
    "spec_dict['lognorm']  = '10log10'\n",
    "spec_dict['vlim']     = 'band' # 'all' or 'band' or 'batchID'\n",
    "spec_dict['cmap']     = 'RdBu_r'\n",
    "spec_dict['figname']  = 'firstLayerActivationsPSD'\n",
    "\n",
    "for batch_ID in batch_IDs:\n",
    "    fig = eegvislib.temporalFM_PSDv2(input_window_normalized, \n",
    "                                     Mdl, layer, \n",
    "                                     spec_dict, inout_labels,\n",
    "                                     batch_ID, FM_ID)\n",
    "    if save_img:\n",
    "        fig.savefig(imgPath + spec_dict['figname'] + f'_batch{batch_ID}'+ f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                    transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7fbf8-1abf-4e71-9f1b-b28b3247abfb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Conv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5ebe20-b774-4295-81db-d4e5fe93a08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'encoder.conv2'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa39ef4-917c-4eb3-93c7-adf0df287589",
   "metadata": {},
   "source": [
    "KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3419ad-9b2d-46fe-ab60-fb91957a9e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_ID    = None #None (all Kernels), a number 0, or a list [0], [0,2]\n",
    "kernel_ID    = [0] #None (all FM), a number 0, or a list [0], [0,2]\n",
    "kernel_width = 0\n",
    "\n",
    "spec_dict = eegvislib.get_spec_dict(spec_dict_keys, spec_dict_bands)\n",
    "spec_dict['figdim']      = [5,5]\n",
    "spec_dict['font']        = 30\n",
    "spec_dict['title']       = 'Second Layer Filters ShallowNet'\n",
    "spec_dict['cbartick']    = 5\n",
    "spec_dict['norm']        = 'nothing' #nothing, or softmax or abs\n",
    "spec_dict['min_softmax'] = 1/len(channels) # 0 or 1/len(list_channels) (random chance)\n",
    "spec_dict['filternames'] = [i for i in spec_dict['bands'].keys()] #or None\n",
    "spec_dict['contours']    = 0\n",
    "spec_dict['vlim']        = [-0.4,0.4]\n",
    "spec_dict['cmap']        = ['RdBu_r','Reds'] #two colormaps, one for weights, and one for abs(weights)\n",
    "spec_dict['figname']     = 'secondLayerFilters'\n",
    "\n",
    "fig = eegvislib.spatialkernels_scalp(Mdl_weights, layer, \n",
    "                                     spec_dict,\n",
    "                                     filter_ID, kernel_ID, kernel_width, \n",
    "                                     None)\n",
    "\n",
    "if save_img:\n",
    "    fig.savefig(imgPath + spec_dict['figname'] + f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd3583-e48d-44c2-b0de-5f69fc11880c",
   "metadata": {},
   "source": [
    "ACTIVATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4e4013-cd66-4285-8dfd-5a7237d14efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "convoluted_output  = eegvislib.out_activation(Mdl, layer, input_window_normalized)\n",
    "convoluted_output.shape\n",
    "#batch x feature maps x scalp channels x time step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c9234-265d-4096-a69f-0876c318e1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_IDs = [100, 1000]\n",
    "FM_ID = None #None (all FM) or a number 0 or list [0], [2,4]\n",
    "\n",
    "spec_dict = eegvislib.get_spec_dict(spec_dict_keys, spec_dict_bands)\n",
    "spec_dict['figdim']   = [10,3]\n",
    "spec_dict['font']     = 14\n",
    "spec_dict['title']    = 'PSD_FM'\n",
    "spec_dict['cbartick'] = 5\n",
    "spec_dict['rotation'] = 45\n",
    "spec_dict['groupby']  = [5,1]\n",
    "spec_dict['lognorm']  = '10log10'\n",
    "spec_dict['cmap']     = 'RdBu_r'\n",
    "spec_dict['figname']  = 'secondLayerActivationsPSD'\n",
    "\n",
    "for batch_ID in batch_IDs:\n",
    "    fig = eegvislib.temporal_FM_PSD_layer2(input_window_normalized, \n",
    "                                           Mdl, layer,\n",
    "                                           spec_dict, inout_labels,\n",
    "                                           batch_ID, FM_ID)\n",
    "\n",
    "    if save_img:\n",
    "        fig.savefig(imgPath + spec_dict['figname'] + f'_batch{batch_ID}'+ f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                    transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88057703-99f5-46d4-a574-132956b27835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "batch_IDs = [100, 1000]\n",
    "FM_ID = None #None #None (all FM) or a number 0 or list [0], [2,4]\n",
    "\n",
    "spec_dict = eegvislib.get_spec_dict(spec_dict_keys, spec_dict_bands)\n",
    "spec_dict['FMnames'] = [i for i in spec_dict['bands'].keys()] #or None\n",
    "spec_dict['backend'] = 'matplotlib' #matplotlib or qt\n",
    "spec_dict['figname'] = 'temporalFMLayer2'\n",
    "\n",
    "for batch_ID in batch_IDs:\n",
    "    fig = eegvislib.temporal_FM_layer2(input_window_normalized,\n",
    "                          Mdl, layer,\n",
    "                          spec_dict,\n",
    "                          batch_ID, FM_ID)\n",
    "    if save_img:\n",
    "        fig.savefig(imgPath + spec_dict['figname'] + f'_batch{batch_ID}'+ f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                    transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049adeb9-ba63-409f-a693-1a60531f1f08",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30759d4-bcde-4b94-be51-5f6b53b6dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'encoder.batch1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c05ba-1f5d-4136-a3da-1349d79213cc",
   "metadata": {},
   "source": [
    "ACTIVATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf05f644-8c4d-4973-9e16-196679ea9425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt\n",
    "batch_IDs = [100, 1000]\n",
    "FM_ID = None #None #None (all FM) or a number 0 or list [0], [2,4]\n",
    "\n",
    "spec_dict = eegvislib.get_spec_dict(spec_dict_keys, spec_dict_bands)\n",
    "spec_dict['FMnames'] = [i for i in spec_dict['bands'].keys()] #or None\n",
    "spec_dict['backend'] = 'matplotlib' #matplotlib or qt\n",
    "spec_dict['figname'] = 'temporalFMLayer2'\n",
    "\n",
    "for batch_ID in batch_IDs:\n",
    "    fig = eegvislib.temporal_FM_layer2(input_window_normalized,\n",
    "                          Mdl, layer,\n",
    "                          spec_dict,\n",
    "                          batch_ID, FM_ID)\n",
    "    if save_img:\n",
    "        fig.savefig(imgPath + spec_dict['figname'] + f'_batch{batch_ID}'+ f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                    transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a06708-5cbd-49b4-b464-00f98d06c9f7",
   "metadata": {},
   "source": [
    "### Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed96a82-a938-499e-a244-068d5fa9dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'Dense'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f9160-31dc-4466-9357-cf52a3dd7312",
   "metadata": {},
   "source": [
    "KERNEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a57ea2f-ac18-4870-92ea-1b3993dbc5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_dict = eegvislib.get_spec_dict(spec_dict_keys, spec_dict_bands)\n",
    "spec_dict['figdim']    = [5,2.5]\n",
    "spec_dict['font']      = 14\n",
    "spec_dict['title']     = f'Dense Layer Weights: Split {outFold}-{inFold}'\n",
    "spec_dict['cbartick']  = 5\n",
    "spec_dict['plot_type'] = 'both'\n",
    "spec_dict['xticks']    = [i for i in spec_dict['bands'].keys()] #or None\n",
    "spec_dict['groupby']   = 1\n",
    "spec_dict['cmap']      = 'RdBu_r'\n",
    "spec_dict['figname']   = 'denseLayerWeights'\n",
    "\n",
    "fig, mask = eegvislib.denseweights_plot(Mdl_weights,layer,\n",
    "                                        spec_dict)\n",
    "\n",
    "if save_img:\n",
    "    fig.savefig(imgPath + spec_dict['figname'] + f'{outFold}_{inFold}' + imgs_format,\n",
    "                transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3fe510-553a-4312-8b03-664031e702d2",
   "metadata": {},
   "source": [
    "ACTIVATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5fb64-0b18-4bb9-af20-cf5b169ad243",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'encoder'\n",
    "convoluted_output = eegvislib.out_activation(Mdl, layer, input_window_normalized)\n",
    "convoluted_output.shape\n",
    "#batch x feature maps x scalp channels x timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d86d72d-d126-48d1-9b00-586f5d08a936",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_IDs = [100, 1000]\n",
    "\n",
    "spec_dict = eegvislib.get_spec_dict(spec_dict_keys, spec_dict_bands)\n",
    "spec_dict['figdim']   = [15,1.5]\n",
    "spec_dict['linew']    = 2\n",
    "spec_dict['font']     = 20\n",
    "spec_dict['rotation'] = 90\n",
    "spec_dict['groupby']  = 1\n",
    "spec_dict['cmap']     = 'RdBu_r'\n",
    "spec_dict['figname']  = 'flattenLayerActivations'\n",
    "\n",
    "for batch_ID in batch_IDs:\n",
    "    fig = eegvislib.flatten_fm(input_window_normalized,\n",
    "                               Mdl, layer, \n",
    "                               spec_dict, inout_labels,\n",
    "                               batch_ID)\n",
    "    if save_img:\n",
    "        fig.savefig(imgPath + spec_dict['figname'] + f'_batch{batch_ID}' + f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                    transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247e057-446c-4db6-a6f4-da3f0713d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_dict = eegvislib.get_spec_dict(spec_dict_keys, spec_dict_bands)\n",
    "spec_dict['figdim']  = [5,5]\n",
    "spec_dict['s']       = 10\n",
    "spec_dict['loc']     = 'upper left'\n",
    "spec_dict['font']    = 20\n",
    "spec_dict['norm']    = 10\n",
    "spec_dict['lognorm'] = '10log10'\n",
    "spec_dict['pval']    = 0.05\n",
    "spec_dict['method']  = 'holm'\n",
    "spec_dict['figname'] = 'flattenCorrelationPSD'\n",
    "\n",
    "fig = eegvislib.flattenCorrelationPSD(input_window_normalized, \n",
    "                                      Mdl, layer,\n",
    "                                      spec_dict)\n",
    "\n",
    "if save_img:\n",
    "    fig.savefig(imgPath + spec_dict['figname'] + f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36389580-3653-430b-b32c-c6ecfaf9a517",
   "metadata": {},
   "source": [
    "## Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1c9eef-5226-4204-ad34-5b3c1c860a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'Dense'\n",
    "input_window_normalized      = testloader.dataset[:][0]\n",
    "convoluted_output_test       = eegvislib.out_activation(Mdl, layer, input_window_normalized)\n",
    "\n",
    "validation_window_normalized = valloader.dataset[:][0]\n",
    "convoluted_output_val        = eegvislib.out_activation(Mdl, layer, validation_window_normalized)\n",
    "\n",
    "training_window_normalized   = trainloader.dataset[:][0]\n",
    "convoluted_output_train      = eegvislib.out_activation(Mdl, layer, training_window_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e4a96-ae51-4160-8911-cbd445ad398e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddingF   = torch.cat((convoluted_output_test, convoluted_output_val, convoluted_output_train), dim=0).numpy()\n",
    "labels_test  = [classlabels[i] for i in testloader.dataset[: ][1].to(device = 'cpu').numpy()]\n",
    "labels_val   = [classlabels[i] for i in valloader.dataset[ : ][1].to(device = 'cpu').numpy()]\n",
    "labels_train = [classlabels[i] for i in trainloader.dataset[:][1].to(device = 'cpu').numpy()]\n",
    "labels       = [labels_test, labels_val, labels_train]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a013d5-c322-406d-b5ce-65055f0dec45",
   "metadata": {},
   "source": [
    "### TRAIN-VALIDATION-TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b778bd92-564e-4605-8943-68ac20d399f6",
   "metadata": {},
   "source": [
    "2D VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ae0712-3c78-4397-a2c8-2daf4a62f6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_dict ={'title': '',\n",
    "            'alpha_trainval': 0.5,\n",
    "            's_trainval': 4,\n",
    "            'alpha_test': 1,\n",
    "            's_test': 10,\n",
    "            'font': 16,\n",
    "            'figdim': [10, 10],\n",
    "            'marker': '.',\n",
    "            'colors': ['tab:orange','blue', 'k'], #TEST, VAL, TRAIN\n",
    "            'colors_lines': ['tab:red','violet', 'gray'], #TEST, VAL, TRAIN\n",
    "            'loc': 'upper left',\n",
    "            'linew': 2,\n",
    "            'level': 0.3173,#0.046, #0.3173\n",
    "            'gridsize': 50j,\n",
    "            'sets': ['TRAIN','VAL','TEST'],\n",
    "            'figname': 'overlap2Dsplit'}\n",
    "\n",
    "# Main figure with subplots\n",
    "dim = [[0, 1], [2, 1], [0, 2]]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10),\n",
    "                         constrained_layout=True)\n",
    "fig.suptitle(f'ACTIVATION 2D: Sets | Split {outFold}-{inFold}',\n",
    "             fontsize=spec_dict['font']+2)\n",
    "\n",
    "for i, ax in enumerate(axes.flat[:3]):\n",
    "    spec_dict['dim'] = dim[i]\n",
    "    spec_dict['xlabel'] = 'Output Activation ' + classlabels[dim[i][0]]\n",
    "    spec_dict['ylabel'] = 'Output Activation ' + classlabels[dim[i][1]]\n",
    "    eegvislib.embedding_split2D(embeddingF[:, dim[i]], labels, \n",
    "                                spec_dict, \n",
    "                                ax=ax)\n",
    "    \n",
    "fig.delaxes(axes[1, 1])\n",
    "\n",
    "if save_img:\n",
    "    fig.savefig(imgPath + spec_dict['figname']+ f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133a2985-463f-4ef3-bd81-ab7836afed20",
   "metadata": {},
   "source": [
    "3D VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c4ed599-fb96-4449-89b1-92e18cb27a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_dict ={'title': f'ACTIVATION 3D: Sets | Split {outFold}-{inFold}',\n",
    "            'alpha_trainval': 1,\n",
    "            's_trainval': 1,\n",
    "            'alpha_test': 1,\n",
    "            's_test': 1,\n",
    "            'font': 16,\n",
    "            'figdim': [12, 12],\n",
    "            'marker': '.',\n",
    "            'loc': 'upper left',\n",
    "            'cmap': ['Greys_r','Reds_r'],\n",
    "            'view': {'elev':None,'azim':None,'roll':None},\n",
    "            'level': 0.3173,\n",
    "            'sigmalevels': np.arange(0,3.5,0.5),\n",
    "            'sigmath': 1, #or None\n",
    "            'gridsize': 50j,\n",
    "            'sets': ['TRAIN','TEST'],\n",
    "            'classlabels': classlabels,\n",
    "            'figname': 'overlap3Dsplit'}\n",
    "\n",
    "#%matplotlib qt\n",
    "\n",
    "fig, ax, overlap = eegvislib.embedding_split3D(embeddingF, labels,\n",
    "                                               spec_dict)\n",
    "print(f'Overlap: {overlap:.1%} of {spec_dict['sets'][1]} set on {spec_dict['sets'][0]} set')\n",
    "\n",
    "if save_img:\n",
    "    fig.savefig(imgPath + spec_dict['figname'] + f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01583fa7-d1af-414c-8d0c-c2fc67aa7b8b",
   "metadata": {},
   "source": [
    "### LABELS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5a7eae-6dde-43d6-8060-3ae5ac295c7b",
   "metadata": {},
   "source": [
    "2D VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfe4411-c5d4-4b46-8ae5-aa53fa132d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_dict = {'title': '',\n",
    "             'alpha':0.5,\n",
    "             's':4,\n",
    "             'alpha_triangle': 0.7,\n",
    "             'font':16,\n",
    "             'figdim':[15,15],\n",
    "             'marker':'.',\n",
    "             'colors':['tab:green','tab:blue','tab:red'],\n",
    "             'loc':'upper left',\n",
    "             'linew': 3,\n",
    "             'level': 0.3173,\n",
    "             'sets': ['TEST','TRAIN'],\n",
    "             'classlabels': classlabels,\n",
    "             'figname': 'overlap2Dlabels'}\n",
    "\n",
    "# Main figure with subplots\n",
    "dim = [[0, 1], [2, 1], [0, 2]]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(spec_dict['figdim'][0], spec_dict['figdim'][1]),\n",
    "                         constrained_layout=True)\n",
    "fig.suptitle(f'ACTIVATION 2D: Labels | Split {outFold}-{inFold}',\n",
    "             fontsize=spec_dict['font']+2)\n",
    "\n",
    "for i, ax in enumerate(axes.flat[:3]):\n",
    "    spec_dict['dim'] = dim[i]\n",
    "    spec_dict['xlabel'] = 'Output Activation ' + classlabels[dim[i][0]]\n",
    "    spec_dict['ylabel'] = 'Output Activation ' + classlabels[dim[i][1]]\n",
    "    eegvislib.embedding_labels2D(embeddingF[:, dim[i]], labels, \n",
    "                                 spec_dict, \n",
    "                                 ax=ax)\n",
    "\n",
    "fig.delaxes(axes[1, 1])\n",
    "\n",
    "if save_img:\n",
    "    fig.savefig(imgPath + spec_dict['figname'] + f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683abe42-8abc-4374-92f1-442225c28bac",
   "metadata": {},
   "source": [
    "3D VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f376f9a-00bd-41e0-bdd0-fd63577c8628",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_dict ={'title': f'ACTIVATION 3D: Labels | Split {outFold}-{inFold}',\n",
    "            'alpha': 0.4,\n",
    "            's': 4,\n",
    "            'alpha_triangle': 0.7,\n",
    "            'font': 16,\n",
    "            'figdim': [12, 12],\n",
    "            'marker': '.',\n",
    "            'colors':['tab:green','tab:blue','tab:red'],\n",
    "            'loc': 'upper left',\n",
    "            'linew': 2,\n",
    "            'sets': ['TRAIN'],\n",
    "            'classlabels': classlabels,\n",
    "            'figname': 'overlap3Dlabels'}\n",
    "\n",
    "#%matplotlib qt\n",
    "fig, ax, baricenters, area = eegvislib.embedding_labels3D(embeddingF, labels,\n",
    "                                                          spec_dict)\n",
    "\n",
    "if save_img:\n",
    "    fig.savefig(imgPath + spec_dict['figname'] + f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972cee74-9a96-48f0-95c3-e6cf01e72970",
   "metadata": {},
   "source": [
    "### TEST SUBJECTS AND CLASSIFICATION MISTAKES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bcfe6b-47ff-4508-81c6-b8c7460e09d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nw_test  = np.concatenate(([testset.EEGcumlen[0]],  np.diff(testset.EEGcumlen)))\n",
    "\n",
    "colors_test = []\n",
    "for i in range(len(nw_test)):\n",
    "    colors_test.extend([plt.cm.tab10.colors[i % 10]]*nw_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "254b0e81-6a13-445d-86e5-e3c985cd9f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_dict = {'title': '',\n",
    "             'alpha_test':1,\n",
    "             'alpha_trainval':0.5,\n",
    "             's_test':20,\n",
    "             's_trainval':4,\n",
    "             'font':16,\n",
    "             'figdim':[15,15],\n",
    "             'marker':'.',\n",
    "             'loc':'lower right',\n",
    "             'loc_subj': 'lower left',\n",
    "             'linew': 2,\n",
    "             'level': 0.3173,\n",
    "             'nsub': (len(partition_list[0][0]) + len(partition_list[0][1]) + len(partition_list[0][2])),\n",
    "             'gridsize': 50j,\n",
    "             'mode': 'subjects', #classification or subjects\n",
    "             'figname': 'overlap2Dclassification'}\n",
    "\n",
    "dim = [[0, 1], [2, 1], [0, 2]]\n",
    "fig, axes = plt.subplots(2, 2, figsize=(spec_dict['figdim'][0], spec_dict['figdim'][1]),\n",
    "                         constrained_layout=True)\n",
    "fig.suptitle(f'ACTIVATION 2D: Classification Mistakes | Split {outFold}-{inFold}',\n",
    "             fontsize=spec_dict['font']+2)\n",
    "\n",
    "for i, ax in enumerate(axes.flat[:3]):\n",
    "    spec_dict['xlabel'] = 'Output Activation ' + classlabels[dim[i][0]]\n",
    "    spec_dict['ylabel'] = 'Output Activation ' + classlabels[dim[i][1]]\n",
    "    eegvislib.embedding_class2D(embeddingF[:,dim[i]], inout_labels,\n",
    "                                spec_dict, \n",
    "                                test_id, nw_test, colors_test, \n",
    "                                ax=ax)\n",
    "\n",
    "fig.delaxes(axes[1, 1])\n",
    "\n",
    "if save_img:\n",
    "    fig.savefig(imgPath + spec_dict['figname']+ f'_split{outFold}_{inFold}' + imgs_format,\n",
    "                transparent=False, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724bcbb2-32ed-4177-a403-7a1b06091928",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(eegvislib)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
