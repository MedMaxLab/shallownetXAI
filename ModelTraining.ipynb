{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "881bf5c6-57ea-4632-bccf-47c3f63822ab",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63813871-4c4d-4c48-a556-9c3f2a196444",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORT LIBRARIES\n",
    "import argparse\n",
    "import glob\n",
    "from itertools import chain, combinations, product\n",
    "import math\n",
    "import importlib\n",
    "import os\n",
    "import random\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "# IMPORT STANDARD PACKAGES\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "from scipy.signal import welch, firwin\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "# IMPORT TORCH\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchaudio import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# IMPORT SELFEEG \n",
    "import selfeeg\n",
    "import selfeeg.models as zoo\n",
    "import selfeeg.dataloading as dl\n",
    "\n",
    "# IMPORT REPOSITORY FUNCTIONS\n",
    "import AllFnc\n",
    "from AllFnc import split\n",
    "from AllFnc.models import ShallowNet2\n",
    "from AllFnc.training import (\n",
    "    loadEEG,\n",
    "    lossBinary,\n",
    "    lossMulti,\n",
    "    train_model,\n",
    "    get_performances,\n",
    "    GetLearningRate)\n",
    "\n",
    "# IMPORT EEGVISLIB\n",
    "from AllFnc import eegvislib\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message = \"Using padding='same'\", category = UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e1b8c2-8173-4ce1-baa1-1960ffa876bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# SET DEVICE: 0 = ZANOLA\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(0)\n",
    "print(device)\n",
    "\n",
    "# SET PATH\n",
    "dataPath = '/data/delpup/datasets/eegpickle/'\n",
    "osPath   = os.path.abspath(os.getcwd())\n",
    "sep      = os.path.sep\n",
    "imgPath  = osPath + sep + 'imgs' + sep"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88296001-c3f7-4176-87fd-068d688ff91a",
   "metadata": {},
   "source": [
    "## Set training spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f3d54d2-202b-4b71-a001-d989a5180bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineToEval = 'filt' #'icasr'\n",
    "taskToEval     = 'alzheimer'\n",
    "modelToEval    = 'shallownet_custom' #shallownet, shallownet_custom \n",
    "outFold        = 6\n",
    "inFold         = 5\n",
    "downsample     = True\n",
    "z_score        = True\n",
    "rem_interp     = True\n",
    "batchsize      = 64\n",
    "overlap        = 0.0\n",
    "workers        = 0\n",
    "window         = 4\n",
    "verbose        = True\n",
    "lr             = 5*10**(-5)\n",
    "epochs         = 1000\n",
    "lossVal        = None\n",
    "arch_acronym   = 'shn7db'\n",
    "seed           = 83136297\n",
    "\n",
    "# ShallowNet\n",
    "# shallownet_custom_dict = {\n",
    "#     \"F1\": 40,\n",
    "#     \"K1\": 25,\n",
    "#     \"F2\": 40,\n",
    "#     \"Pool\": 75,\n",
    "#     \"p\": 0.2,\n",
    "#     \"log_activation_base\": \"e\",\n",
    "#     \"norm_type\": \"batchnorm\",\n",
    "#     \"random_temporal_filter\": True,\n",
    "#     \"Fs\": 125 if downsample else 250 ,\n",
    "#     \"freeze_temporal\": 0,\n",
    "#     \"dense_hidden\": None,\n",
    "#     \"spatial_depthwise\": False,\n",
    "#     \"spatial_only_positive\": False,\n",
    "#     \"global_pooling\": False,\n",
    "#     \"return_logits\": True,\n",
    "#     \"seed\": 83136297\n",
    "# }\n",
    "\n",
    "# Med-ShallowNet\n",
    "shallownet_custom_dict = {\n",
    "    \"F1\": 7,\n",
    "    \"K1\": 125,\n",
    "    \"F2\": 7,\n",
    "    \"Pool\": 75,\n",
    "    \"p\": 0.2,\n",
    "    \"log_activation_base\": \"dB\",\n",
    "    \"norm_type\": \"batchnorm\",\n",
    "    \"random_temporal_filter\": False,\n",
    "    \"Fs\": 125 if downsample else 250 ,\n",
    "    \"freeze_temporal\": 999999999,\n",
    "    \"dense_hidden\": None,\n",
    "    \"spatial_depthwise\": True,\n",
    "    \"spatial_only_positive\": False,\n",
    "    \"global_pooling\": True,\n",
    "    \"bias\": [False, False, False],\n",
    "    \"return_logits\": True,\n",
    "    \"seed\": seed\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88f76699-15f2-4664-85c4-90f7501182fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold to eval is the correct index to get the desired train/val/test part\n",
    "outerFold  = outFold - 1\n",
    "innerFold  = inFold  - 1\n",
    "foldToEval = outerFold*5 + innerFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054cd00e-2936-450c-977d-d40604364be3",
   "metadata": {},
   "source": [
    "## Create partition list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9e2cdff-120f-4808-aaa9-2bfc19e4109e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'alzheimer' in taskToEval.casefold():\n",
    "    # ALZ = subjects 1 to 36; CTL = subjects 37 to 65; FTD = subjects 66 to 88\n",
    "    a_id = [i for i in range(1,37)]\n",
    "    c_id = [i for i in range(37,66)]\n",
    "    f_id = [i for i in range(66,89)]\n",
    "    \n",
    "    part_a = split.create_nested_kfold_subject_split(a_id, 10, 5)\n",
    "    part_c = split.create_nested_kfold_subject_split(c_id, 10, 5)\n",
    "    part_f = split.create_nested_kfold_subject_split(f_id, 10, 5)\n",
    "\n",
    "    if taskToEval.casefold() == 'alzheimer':\n",
    "        partition_list_1 = split.merge_partition_lists(part_a, part_c, 10, 5)\n",
    "        partition_list = split.merge_partition_lists(partition_list_1, part_f, 10, 5)\n",
    "    elif taskToEval.casefold() == 'alzheimerca':\n",
    "        partition_list = split.merge_partition_lists(part_a, part_c, 10, 5)\n",
    "    elif taskToEval.casefold() == 'alzheimercf':\n",
    "        partition_list = split.merge_partition_lists(part_c, part_f, 10, 5)\n",
    "    elif taskToEval.casefold() == 'alzheimeraf':\n",
    "        partition_list = split.merge_partition_lists(part_a, part_f, 10, 5)\n",
    "\n",
    "elif taskToEval.casefold() == 'cognitive':\n",
    "    # CTL = subjects 101 to 149; PD/PDD/PDMCI = mixing number in [1; 100]\n",
    "    c_id = [i for i in range(101,150)]\n",
    "    pd_id = [3, 6, 7, 16, 17, 18, 21, 24, 26, 27, 30, 32, 35, 37, 39, 40, 45,\n",
    "             46, 50, 51, 53, 57, 58, 60, 61, 62, 63, 65, 67, 68, 69, 71, 74,\n",
    "             76, 80, 81, 82, 83, 84, 85, 86, 87, 90, 92, 93, 94, 100]\n",
    "    pdd_id = [1, 2, 5, 8, 9, 12, 13, 15, 22, 25, 33, 38, 44, 48, 75, 78, 88, 95, 96]\n",
    "    pdmci_id = [4, 10, 11, 14, 19, 20, 23, 28, 29, 31, 34, 36, 41, 42, 43, 47,\n",
    "                49, 52, 54, 55, 56, 59, 64, 66, 70, 72, 73, 77, 79, 89, 91, 97,\n",
    "                98, 99]\n",
    "    part_c = split.create_nested_kfold_subject_split(c_id, 10, 5)\n",
    "    part_p = split.create_nested_kfold_subject_split(pd_id, 10, 5)\n",
    "    part_d = split.create_nested_kfold_subject_split(pdd_id, 10, 5)\n",
    "    part_m = split.create_nested_kfold_subject_split(pdmci_id, 10, 5)\n",
    "    \n",
    "    # first --> mix two groups; then --> mix the mix\n",
    "    # splits have a more similar number of subject per set in this way\n",
    "    partition_1 = split.merge_partition_lists(part_c, part_m, 10, 5)\n",
    "    partition_2 = split.merge_partition_lists(part_p, part_d, 10, 5)\n",
    "    partition_list = split.merge_partition_lists(partition_1, partition_2, 10, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90352f5-9a32-4af2-add6-91d9368cd17e",
   "metadata": {},
   "source": [
    "## Get other dataset info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eb580170-767d-4cd6-8936-cc18fa860ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Path to EEG data as a concatenation of:\n",
    "# 1) the root path\n",
    "# 2) the preprocessing pipeline\n",
    "if dataPath[-1] != os.sep:\n",
    "    dataPath += os.sep\n",
    "if pipelineToEval[-1] != os.sep:\n",
    "    eegpath = dataPath + pipelineToEval + os.sep\n",
    "else:\n",
    "    eegpath = dataPath + pipelineToEval\n",
    "\n",
    "# Define the number of Channels to use. \n",
    "# Basically 61 due to BIDSAlign channel system alignment.\n",
    "# Note that BIDSAlign DOES NOT delete any original channel by default.\n",
    "if rem_interp:\n",
    "    if 'alzheimer' in taskToEval.casefold():\n",
    "        Chan = 19\n",
    "    elif taskToEval.casefold() == 'cognitive':\n",
    "        Chan = 59\n",
    "else:\n",
    "    Chan = 61\n",
    "\n",
    "# Define the sampling rate. 125 or 250 depending on the downsample option\n",
    "srate = 125 if downsample else 250\n",
    "\n",
    "# Define the number of classes to predict.\n",
    "# All tasks are binary except the Alzheimer's one, \n",
    "# which is a multi-class classification (Alzheimer vs FrontoTemporal vs Control)\n",
    "if taskToEval.casefold() == 'alzheimer':\n",
    "    nb_classes = 3\n",
    "elif taskToEval.casefold() == 'cognitive':\n",
    "    nb_classes = 4\n",
    "else:\n",
    "    nb_classes = 2\n",
    "\n",
    "# For selfEEG's models instantiation\n",
    "Samples = int(srate*window)\n",
    "\n",
    "# Set the Dataset ID for glob.glob operation in SelfEEG's GetEEGPartitionNumber().\n",
    "if 'alzheimer' in taskToEval.casefold():\n",
    "    datasetID = '10'\n",
    "elif taskToEval.casefold() == 'cognitive':\n",
    "    datasetID = '19'\n",
    "\n",
    "# Set the class label in case of plot of functions\n",
    "if taskToEval.casefold() == 'alzheimer':\n",
    "    classlabels = ['CTL', 'FTD', 'AD']\n",
    "elif taskToEval.casefold() == 'alzheimerca':\n",
    "    classlabels = ['CTL', 'AD']\n",
    "elif taskToEval.casefold() == 'alzheimercf':\n",
    "    classlabels = ['CTL', 'FTD']\n",
    "elif taskToEval.casefold() == 'alzheimeraf':\n",
    "    classlabels = ['FTD', 'AD']\n",
    "elif taskToEval.casefold() == 'cognitive':\n",
    "    classlabels = ['CTL', 'PD', 'PDD', 'PDMCI']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed98384d-b520-4753-9bf4-a3edd2e3542a",
   "metadata": {},
   "source": [
    "## Define torch dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61b949c8-8648-4799-aa0a-d4a2571fbb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting EEG samples: 100%|███████████████| 88/88 [00:04<00:00, 21.64 files/s]\n",
      "\n",
      "Concluded extraction of repository length with the following specific: \n",
      "\n",
      "window          ==>  4.00 s\n",
      "overlap         ==>  0.00 %\n",
      "sampling rate   ==> 125.00 Hz\n",
      "-----------------------------\n",
      "dataset length  ==>    17604\n",
      " \n",
      "Subjects used for test\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 58\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSubjects used for test\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28mprint\u001b[39m(test_id)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_id' is not defined"
     ]
    }
   ],
   "source": [
    "loadEEG_args = {'return_label': False, \n",
    "                'downsample': downsample, \n",
    "                'use_only_original': rem_interp,\n",
    "                'apply_zscore': z_score}\n",
    "\n",
    "glob_input = [datasetID + '_*.pickle']\n",
    "\n",
    "# calculate dataset length.\n",
    "# Basically it automatically retrieves all the partitions \n",
    "# that can be extracted from each EEG signal\n",
    "EEGlen = dl.get_eeg_partition_number(\n",
    "    eegpath,\n",
    "    srate,\n",
    "    window,\n",
    "    overlap, \n",
    "    file_format = glob_input,\n",
    "    load_function = loadEEG,\n",
    "    optional_load_fun_args = loadEEG_args,\n",
    "    includePartial = False if overlap == 0 else True,\n",
    "    verbose = verbose\n",
    ")\n",
    "\n",
    "# Now we also need to load the labels\n",
    "loadEEG_args['return_label'] = True\n",
    "\n",
    "# Set functions to retrieve dataset, subject, and session from each filename.\n",
    "# They will be used by GetEEGSplitTable to perform a subject based split\n",
    "dataset_id_ex  = lambda x: int(x.split(os.sep)[-1].split('_')[0])\n",
    "subject_id_ex  = lambda x: int(x.split(os.sep)[-1].split('_')[1]) \n",
    "session_id_ex  = lambda x: int(x.split(os.sep)[-1].split('_')[2]) \n",
    "\n",
    "# Now call the GetEEGSplitTable. \n",
    "if taskToEval.casefold() == 'alzheimerca':\n",
    "    exclude_id = f_id\n",
    "elif taskToEval.casefold() == 'alzheimercf':\n",
    "    exclude_id = a_id\n",
    "elif taskToEval.casefold() == 'alzheimeraf':\n",
    "    exclude_id = c_id\n",
    "else:\n",
    "    exclude_id = None\n",
    "        \n",
    "    train_id   = partition_list[foldToEval][0]\n",
    "    val_id     = partition_list[foldToEval][1]\n",
    "    test_id    = partition_list[foldToEval][2]\n",
    "    EEGsplit = dl.get_eeg_split_table(\n",
    "        partition_table      = EEGlen,\n",
    "        val_data_id          = val_id,\n",
    "        test_data_id         = test_id,\n",
    "        exclude_data_id      = exclude_id,\n",
    "        split_tolerance      = 0.001,\n",
    "        dataset_id_extractor = subject_id_ex,\n",
    "        subject_id_extractor = session_id_ex,\n",
    "        perseverance         = 10000)\n",
    "\n",
    "if verbose:\n",
    "    print(' ')\n",
    "    print('Subjects used for test')\n",
    "    print(test_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740546fa-6167-4e17-8f66-730eb03386f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "train ratio:      0.75\n",
      "validation ratio: 0.16\n",
      "test ratio:       0.10\n",
      "\n",
      "train labels ratio: 0.0 = 0.341 ,  1.0 = 0.220 ,  2.0 = 0.439 , \n",
      "val   labels ratio: 0.0 = 0.364 ,  1.0 = 0.265 ,  2.0 = 0.371 , \n",
      "test  labels ratio: 0.0 = 0.360 ,  1.0 = 0.324 ,  2.0 = 0.316 , \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define Datasets and preload all data\n",
    "trainset = dl.EEGDataset(\n",
    "    EEGlen, EEGsplit, [srate, window, overlap], 'train', \n",
    "    supervised             = True, \n",
    "    label_on_load          = True,\n",
    "    load_function          = loadEEG,\n",
    "    optional_load_fun_args = loadEEG_args\n",
    ")\n",
    "trainset.preload_dataset()\n",
    "\n",
    "valset = dl.EEGDataset(\n",
    "    EEGlen, EEGsplit, [srate, window, overlap], 'validation',\n",
    "    supervised             = True, \n",
    "    label_on_load          = True,\n",
    "    load_function          = loadEEG,\n",
    "    optional_load_fun_args = loadEEG_args\n",
    ")\n",
    "valset.preload_dataset()\n",
    "\n",
    "testset = dl.EEGDataset(\n",
    "    EEGlen, EEGsplit, [srate, window, overlap], 'test',\n",
    "    supervised             = True,\n",
    "    label_on_load          = True,\n",
    "    load_function          = loadEEG,\n",
    "    optional_load_fun_args = loadEEG_args\n",
    ")\n",
    "testset.preload_dataset()\n",
    "\n",
    "# Convert to long if task is multiclass classification.\n",
    "# This avoids Value Errors during cross entropy loss calculation\n",
    "if ('alzheimer' in taskToEval.casefold()) or ('cognitive' in taskToEval.casefold()):\n",
    "    if taskToEval.casefold() == 'alzheimerca':\n",
    "        trainset.y_preload[trainset.y_preload==2] = 1\n",
    "        valset.y_preload[valset.y_preload==2] = 1 \n",
    "        testset.y_preload[testset.y_preload==2] = 1\n",
    "        \n",
    "    elif taskToEval.casefold() == 'alzheimeraf':\n",
    "        trainset.y_preload -= 1\n",
    "        valset.y_preload   -= 1 \n",
    "        testset.y_preload  -= 1\n",
    "        \n",
    "    elif taskToEval.casefold() == 'alzheimeraf':\n",
    "        pass\n",
    "        \n",
    "    else:\n",
    "        trainset.y_preload = trainset.y_preload.to(dtype = torch.long)\n",
    "        valset.y_preload   = valset.y_preload.to(dtype = torch.long)\n",
    "        testset.y_preload  = testset.y_preload.to(dtype = torch.long)\n",
    "    \n",
    "# Finally, Define Dataloaders\n",
    "# (no need to use more workers in validation and test dataloaders)\n",
    "trainloader = DataLoader(dataset = trainset, batch_size = batchsize,\n",
    "                         shuffle = True, num_workers = workers)\n",
    "valloader = DataLoader(dataset = valset, batch_size = batchsize,\n",
    "                       shuffle = False, num_workers = 0)\n",
    "testloader = DataLoader(dataset = testset, batch_size = batchsize,\n",
    "                        shuffle = False, num_workers = 0)\n",
    "\n",
    "if verbose:\n",
    "    # plot split statistics\n",
    "    labels = np.zeros(len(EEGlen))\n",
    "    for i in range(len(EEGlen)):\n",
    "        path = EEGlen.iloc[i,0]\n",
    "        with open(path, 'rb') as eegfile:\n",
    "            EEG = pickle.load(eegfile)\n",
    "        labels[i] = EEG['label']\n",
    "    dl.check_split(EEGlen, EEGsplit, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500cd394-56d3-4016-b824-78a8a3eb9a54",
   "metadata": {},
   "source": [
    "## Define loss, model, and other training parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a98578da-9570-4440-8c88-39ed0c1360d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "                 Modules  Parameters\n",
      "0   encoder.conv1.weight        1000\n",
      "1     encoder.conv1.bias          40\n",
      "2   encoder.conv2.weight       30400\n",
      "3     encoder.conv2.bias          40\n",
      "4  encoder.batch1.weight          40\n",
      "5    encoder.batch1.bias          40\n",
      "6           Dense.weight        3240\n",
      "7             Dense.bias           3\n",
      "====================================\n",
      "  TOTAL TRAINABLE PARAMS       34803\n",
      " \n"
     ]
    }
   ],
   "source": [
    "validation_loss_args = []\n",
    "if taskToEval.casefold()==\"alzheimer\" or ('cognitive' in taskToEval.casefold()):\n",
    "    lossFnc = lossMulti\n",
    "else:\n",
    "    lossFnc = lossBinary\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "# define model\n",
    "Mdl = ShallowNet2(nb_classes, Chan, Samples, **shallownet_custom_dict)\n",
    "MdlBase = copy.deepcopy(Mdl)\n",
    "\n",
    "Mdl.to(device = device)\n",
    "Mdl.train()\n",
    "if verbose:\n",
    "    print(' ')\n",
    "    ParamTab = selfeeg.utils.count_parameters(Mdl, False, True, True)\n",
    "    print(' ')\n",
    "\n",
    "if lr == 0:\n",
    "    lr = GetLearningRate(modelToEval, taskToEval)\n",
    "    if verbose:\n",
    "        print(' ')\n",
    "        print('used learning rate', lr)\n",
    "        \n",
    "optimizer = torch.optim.Adam(Mdl.parameters(), lr = lr)\n",
    "gamma = 0.995\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma = gamma)\n",
    "\n",
    "#factor = 0.5\n",
    "#patience_scheduler = 10\n",
    "#scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=factor, patience=patience_scheduler)\n",
    "\n",
    "# Define selfEEG's EarlyStopper with large patience to act as a model checkpoint\n",
    "earlystop = selfeeg.ssl.EarlyStopping(\n",
    "    patience = 15, \n",
    "    min_delta = 1e-04, \n",
    "    record_best_weights = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373a059a-4f9d-4cbe-8217-88048fb23657",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89911b2c-8452-4dda-9d01-74b64d7f9751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch [1/1000]\n",
      "Epoch 0: Balanced Accuracy = 50.89% | 205/250 [75.18 Batch/s, train_loss=0.93351, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [66.59 Batch/s, train_loss=0.93351, val_loss=0.88031]  \n",
      "epoch [2/1000]\n",
      "Epoch 1: Balanced Accuracy = 64.69% | 205/250 [72.87 Batch/s, train_loss=0.69728, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [82.85 Batch/s, train_loss=0.69728, val_loss=0.95355]  \n",
      "epoch [3/1000]\n",
      "Epoch 2: Balanced Accuracy = 74.71% | 205/250 [75.11 Batch/s, train_loss=0.53478, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [86.91 Batch/s, train_loss=0.53478, val_loss=0.99563]  \n",
      "epoch [4/1000]\n",
      "Epoch 3: Balanced Accuracy = 82.63% | 205/250 [77.97 Batch/s, train_loss=0.41545, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [89.30 Batch/s, train_loss=0.41545, val_loss=1.07336]  \n",
      "epoch [5/1000]\n",
      "Epoch 4: Balanced Accuracy = 87.61% | 205/250 [74.65 Batch/s, train_loss=0.33141, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [85.78 Batch/s, train_loss=0.33141, val_loss=1.18397]  \n",
      "epoch [6/1000]\n",
      "Epoch 5: Balanced Accuracy = 91.17% | 205/250 [73.70 Batch/s, train_loss=0.26531, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [86.01 Batch/s, train_loss=0.26531, val_loss=1.17380]  \n",
      "epoch [7/1000]\n",
      "Epoch 6: Balanced Accuracy = 92.99% | 205/250 [72.92 Batch/s, train_loss=0.22201, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [84.23 Batch/s, train_loss=0.22201, val_loss=1.39224]  \n",
      "epoch [8/1000]\n",
      "Epoch 7: Balanced Accuracy = 94.64% | 205/250 [78.93 Batch/s, train_loss=0.18793, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [89.70 Batch/s, train_loss=0.18793, val_loss=1.26628]  \n",
      "epoch [9/1000]\n",
      "Epoch 8: Balanced Accuracy = 95.04% | 205/250 [78.81 Batch/s, train_loss=0.16693, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [91.16 Batch/s, train_loss=0.16693, val_loss=1.54636]  \n",
      "epoch [10/1000]\n",
      "Epoch 9: Balanced Accuracy = 95.73% | 205/250 [78.51 Batch/s, train_loss=0.14888, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [91.27 Batch/s, train_loss=0.14888, val_loss=1.47254]  \n",
      "epoch [11/1000]\n",
      "Epoch 10: Balanced Accuracy = 96.58%| 205/250 [84.20 Batch/s, train_loss=0.13132, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [93.66 Batch/s, train_loss=0.13132, val_loss=1.55171]  \n",
      "epoch [12/1000]\n",
      "Epoch 11: Balanced Accuracy = 96.51%| 205/250 [77.69 Batch/s, train_loss=0.12616, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [88.66 Batch/s, train_loss=0.12616, val_loss=1.44678]  \n",
      "epoch [13/1000]\n",
      "Epoch 12: Balanced Accuracy = 96.84%| 205/250 [77.57 Batch/s, train_loss=0.11390, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [89.53 Batch/s, train_loss=0.11390, val_loss=1.58906]  \n",
      "epoch [14/1000]\n",
      "Epoch 13: Balanced Accuracy = 97.72%| 205/250 [77.14 Batch/s, train_loss=0.10066, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [89.53 Batch/s, train_loss=0.10066, val_loss=1.59058]  \n",
      "epoch [15/1000]\n",
      "Epoch 14: Balanced Accuracy = 97.55%| 205/250 [86.26 Batch/s, train_loss=0.09498, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [100.60 Batch/s, train_loss=0.09498, val_loss=1.57115] \n",
      "epoch [16/1000]\n",
      "Epoch 15: Balanced Accuracy = 97.87%| 205/250 [85.55 Batch/s, train_loss=0.08843, val_loss=0.00000]\n",
      "   val 44/44: 100%|███████████████| 250/250 [98.36 Batch/s, train_loss=0.08843, val_loss=1.76934]  \n",
      "no improvement after 15 epochs. Training stopped\n"
     ]
    }
   ],
   "source": [
    "loss_summary=train_model(model                 = Mdl,\n",
    "                        train_dataloader      = trainloader,\n",
    "                        epochs                = epochs,\n",
    "                        optimizer             = optimizer,\n",
    "                        loss_func             = lossFnc, \n",
    "                        lr_scheduler          = scheduler,\n",
    "                        EarlyStopper          = earlystop,\n",
    "                        validation_dataloader = valloader,\n",
    "                        validation_loss_func  = lossVal,\n",
    "                        validation_loss_args  = validation_loss_args,\n",
    "                        verbose               = verbose,\n",
    "                        device                = device,\n",
    "                        return_loss_info      = True\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86bcce2-bb1a-4dbd-98a7-b5c2060b26fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc4cca06-a4ca-4b6e-9442-b73196532f88",
   "metadata": {},
   "source": [
    "## Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5dd41d-79d5-4be6-a64f-9c03ba7e8700",
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop.restore_best_weights(Mdl)\n",
    "Mdl.to(device=device)\n",
    "Mdl.eval()\n",
    "scores = get_performances(loader2eval    = testloader, \n",
    "                          Model          = Mdl, \n",
    "                          device         = device,\n",
    "                          nb_classes     = nb_classes,\n",
    "                          return_scores  = True,\n",
    "                          verbose        = verbose,\n",
    "                          plot_confusion = True,\n",
    "                          class_labels   = classlabels\n",
    "                         )\n",
    "print(f\"Balanced Accuracy: {scores['accuracy_weighted']*100: .2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60fce6f-d58b-420e-b1b0-7db547a06f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_loss_curve = []\n",
    "validation_loss_curve = []\n",
    "# Iterate through the dictionary, filtering out entries with None values\n",
    "for key, (train_loss, val_loss) in loss_summary.items():\n",
    "    if train_loss is not None and val_loss is not None:\n",
    "        training_loss_curve.append(train_loss)\n",
    "        validation_loss_curve.append(val_loss)\n",
    "        \n",
    "scores['training_loss_curve']   = training_loss_curve\n",
    "scores['validation_loss_curve'] = validation_loss_curve\n",
    "\n",
    "plt.plot(training_loss_curve,   label='Training Loss')\n",
    "plt.plot(validation_loss_curve, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Cross-Entropy Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c26420d-197d-4c61-a59a-6f09e768898d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mdl_weights = {k: v.cpu() for k, v in Mdl.state_dict().items()}\n",
    "\n",
    "# Convert to a regular dictionary if needed\n",
    "Mdl_weights = dict(Mdl_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b480703-f05f-4637-918d-e7c6e4f039f3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Initial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9320ea-bf4d-4fc1-8865-facbb99adae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MdlBase.to(device=device)\n",
    "MdlBase.eval()\n",
    "scoresBase = get_performances(loader2eval    = testloader, \n",
    "                              Model          = MdlBase, \n",
    "                              device         = device,\n",
    "                              nb_classes     = nb_classes,\n",
    "                              return_scores  = True,\n",
    "                              verbose        = verbose,\n",
    "                              plot_confusion = True,\n",
    "                              class_labels   = classlabels\n",
    "                             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23c7c77-3e8f-4b82-a4e5-ea2e1d37fcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MdlBase_weights = {k: v.cpu() for k, v in MdlBase.state_dict().items()}\n",
    "\n",
    "# Convert to a regular dictionary if needed\n",
    "MdlBase_weights = dict(MdlBase_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d104378-d818-410c-a4cf-25422dcf0f2b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Comparison with Statistical Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087c8568-7c6a-455c-a5f7-570391b1b3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 'encoder'\n",
    "Mdl.to('cpu')\n",
    "\n",
    "test_window_normalized       = testloader.dataset[:][0].to( device = 'cpu')\n",
    "training_window_normalized   = trainloader.dataset[:][0].to(device = 'cpu')\n",
    "validation_window_normalized = valloader.dataset[:][0].to(  device = 'cpu')\n",
    "y_train = trainloader.dataset[:][1].to(device = 'cpu').numpy()\n",
    "y_test  = testloader.dataset[:][1].to(device = 'cpu').numpy()\n",
    "\n",
    "f_desired = [1, 4, 8, 12, 16, 20, 28, 45]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3424d7-b4e1-47e5-a541-8409489a285d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f,Pxx = eegvislib.get_Pxx(test_window_normalized[0,:,:],srate=srate)\n",
    "findex = eegvislib.get_freq_index(f, f_desired)\n",
    "band_content = np.zeros((test_window_normalized.shape[0],shallownet_custom_dict['F1']))\n",
    "for i in range(test_window_normalized.shape[0]):\n",
    "    f,Pxx = eegvislib.get_Pxx(test_window_normalized[i,:,:],srate=srate)\n",
    "\n",
    "    for j in range(shallownet_custom_dict['F1']):\n",
    "        #bandc = np.log10(np.mean(np.trapz(Pxx[findex[j]:findex[j+1],:].T,dx=(f[1]-f[0]))/(f_desired[j+1]-f_desired[j])))\n",
    "        bandc = 10*np.log10(np.mean(np.trapz(Pxx[findex[j]:findex[j+1],:].T,dx=(f[1]-f[0]))/1))\n",
    "        band_content[i,j] = bandc\n",
    "\n",
    "f,Pxx = eegvislib.get_Pxx(training_window_normalized[0,:,:],srate=srate)\n",
    "band_content_training = np.zeros((training_window_normalized.shape[0],shallownet_custom_dict['F1']))\n",
    "for i in range(training_window_normalized.shape[0]):\n",
    "    f,Pxx = eegvislib.get_Pxx(training_window_normalized[i,:,:],srate=srate)\n",
    "\n",
    "    for j in range(shallownet_custom_dict['F1']):\n",
    "        #bandc = np.log10(np.mean(np.trapz(Pxx[findex[j]:findex[j+1],:].T,dx=(f[1]-f[0]))/(f_desired[j+1]-f_desired[j])))\n",
    "        bandc = 10*np.log10(np.mean(np.trapz(Pxx[findex[j]:findex[j+1],:].T,dx=(f[1]-f[0]))/1))\n",
    "        band_content_training[i,j] = bandc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d0118f-2acd-4510-83bb-63fcbeca6402",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression Model (MLRM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e8bb24-a4b6-4af7-8d6b-262e2b9a45dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = band_content_training\n",
    "X_test  = band_content\n",
    "\n",
    "print('Reference Class: 0 = CTL')\n",
    "model = sm.MNLogit(y_train, X_train)\n",
    "result = model.fit(method='ncg')\n",
    "\n",
    "print(result.summary())\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = result.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "accuracy = balanced_accuracy_score(y_test, predicted_classes)\n",
    "print(f\"Balanced Accuracy: {accuracy*100: .2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a257b-6e08-44ff-93e3-be032e7bfaa2",
   "metadata": {},
   "source": [
    "### shnMLRM: ShallowNet Encoder + MLRM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f541384-0312-499f-bd01-6a22aefcfe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "convoluted_output_test       = eegvislib.out_activation(Mdl, layer, test_window_normalized)\n",
    "convoluted_output_val        = eegvislib.out_activation(Mdl, layer, validation_window_normalized)\n",
    "convoluted_output_train      = eegvislib.out_activation(Mdl, layer, training_window_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff3b51-ef00-4526-922f-082fbcfe0a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = convoluted_output_train.to(device = 'cpu').numpy()\n",
    "X_test  = convoluted_output_test.to(device = 'cpu').numpy()\n",
    "\n",
    "print('Reference Class: 0 = CTL')\n",
    "model = sm.MNLogit(y_train, X_train)\n",
    "result = model.fit(method='ncg')\n",
    "\n",
    "print(result.summary())\n",
    "\n",
    "# Predict on the test data\n",
    "predictions = result.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "accuracy = balanced_accuracy_score(y_test, predicted_classes)\n",
    "print(f\"Balanced Accuracy: {accuracy*100: .2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370a61f-c924-46fe-b057-d529d93a80d4",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Overlap Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75ad087-eafb-4f93-8785-1c0f003712e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test, validation, and training data only once and stay on GPU if possible\n",
    "layer = 'Dense'\n",
    "Mdl.to('cpu')\n",
    "datasets           = [testloader, valloader, trainloader]\n",
    "convoluted_outputs = [eegvislib.out_activation(Mdl, layer, ds.dataset[:][0].to(device='cpu')) for ds in datasets]\n",
    "embeddingF         = torch.cat(convoluted_outputs, dim=0).cpu().numpy()\n",
    "\n",
    "test_samples       = len(testloader.dataset[:][1])\n",
    "val_samples        = len(valloader.dataset[:][1])\n",
    "train_samples      = len(trainloader.dataset[:][1])\n",
    "\n",
    "# Constants and settings\n",
    "prob = 0.6827\n",
    "grid_size = 50j  # Grid resolution for KDE calculation\n",
    "\n",
    "overlap_traintest = eegvislib.get_overlap3D(embeddingF, [test_samples + val_samples,-1], [0,test_samples],\n",
    "                                            prob=prob, grid_size=grid_size)\n",
    "\n",
    "overlap_trainval  = eegvislib.get_overlap3D(embeddingF, [test_samples + val_samples,-1], [test_samples, test_samples + val_samples],\n",
    "                                            prob=prob, grid_size=grid_size)\n",
    "\n",
    "overlap_valtest   = eegvislib.get_overlap3D(embeddingF, [test_samples, test_samples + val_samples], [0, test_samples],\n",
    "                                            prob=prob, grid_size=grid_size)\n",
    "\n",
    "print('Overlap between Sets in the logit space')\n",
    "print(f'Overlap train-test: {overlap_traintest:.2%}, Overlap train-val: {overlap_trainval:.2%}, Overlap val-test: {overlap_valtest:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0205f1a2-574b-4643-a6da-6d8d8532857a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c797b9ee-1428-4923-b07c-a7c3a4b2d584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if taskToEval.casefold() == 'alzheimer':\n",
    "#     start_piece_mdl = 'AlzClassification/Models/'\n",
    "#     start_piece_res = 'AlzClassification/Results/'\n",
    "#     task_piece = 'alz'\n",
    "# elif taskToEval.casefold() == 'alzheimerca':\n",
    "#     start_piece_mdl = 'AlzCAClassification/Models/'\n",
    "#     start_piece_res = 'AlzCAClassification/Results/'\n",
    "#     task_piece = 'al1'\n",
    "# elif taskToEval.casefold() == 'alzheimer':\n",
    "#     start_piece_mdl = 'AlzCFClassification/Models/'\n",
    "#     start_piece_res = 'AlzCFClassification/Results/'\n",
    "#     task_piece = 'al2'\n",
    "# elif taskToEval.casefold() == 'alzheimer':\n",
    "#     start_piece_mdl = 'AlzAFClassification/Models/'\n",
    "#     start_piece_res = 'AlzAFClassification/Results/'\n",
    "#     task_piece = 'al3'\n",
    "\n",
    "# mdl_piece = modelToEval.casefold()\n",
    "    \n",
    "# if pipelineToEval.casefold() == 'raw':\n",
    "#     pipe_piece = 'raw'\n",
    "# elif pipelineToEval.casefold() == 'filt':\n",
    "#     pipe_piece = 'flt'\n",
    "# elif pipelineToEval.casefold() == 'ica':\n",
    "#     pipe_piece = 'ica'\n",
    "# elif pipelineToEval.casefold() == 'icasr':\n",
    "#     pipe_piece = 'isr'\n",
    "\n",
    "# if downsample:\n",
    "#     freq_piece = '125'\n",
    "# else:\n",
    "#     freq_piece = '250'\n",
    "\n",
    "# out_piece = str(outerFold+1).zfill(3)\n",
    "# in_piece = str(innerFold+1).zfill(3)\n",
    "# lr_piece = str(int(lr*1e6)).zfill(6)\n",
    "# chan_piece = str(Chan).zfill(3)\n",
    "# win_piece = str(round(window)).zfill(3)\n",
    "\n",
    "# file_name = '_'.join(\n",
    "#     [task_piece, pipe_piece, freq_piece, mdl_piece, \n",
    "#      out_piece, in_piece, lr_piece, chan_piece, win_piece,\n",
    "#      subj_piece, loss_piece, head_piece\n",
    "#     ]\n",
    "# )\n",
    "# model_path = start_piece_mdl + file_name + '.pt'\n",
    "# results_path = start_piece_res + file_name + '.pickle'\n",
    "\n",
    "# if verbose:\n",
    "#     print('saving model and results in the following paths')\n",
    "#     print(model_path)\n",
    "#     print(results_path)\n",
    "\n",
    "# # Save the model\n",
    "# Mdl.eval()\n",
    "# Mdl.to(device='cpu')\n",
    "# torch.save(Mdl.state_dict(), model_path)\n",
    "\n",
    "# # Save the scores\n",
    "# with open(results_path, 'wb') as handle:\n",
    "#     pickle.dump(scores, handle, protocol = pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# if verbose:\n",
    "#     print('run complete')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
